{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6-PRUNING/6_5_pruning_depth_st_llama3.2-1b_OK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT0FAsUnncFY"
      },
      "source": [
        "<div>\n",
        "    <h1>Large Language Models Projects</a></h1>\n",
        "    <h3>Apply and Implement Strategies for Large Language Models</h3>\n",
        "    <h2>Pruning Llama 3.2.</h2>\n",
        "    <h3>Example of approach to depth pruning a Llama Model.</h3>\n",
        "</div>\n",
        "\n",
        "by [Pere Martra](https://www.linkedin.com/in/pere-martra/)\n",
        "\n",
        "_______\n",
        "Models: meta-llama/Llama-3.2-1B\n",
        "\n",
        "Colab Environment: GPU T4.\n",
        "\n",
        "Keys:\n",
        "* Pruning\n",
        "* Structured pruning\n",
        "* Depth pruning.\n",
        "\n",
        "\n",
        "Related article:\n",
        "_______\n",
        "<table style=\"border: none; background: none;\">\n",
        "  <tr style=\"border: none;\">\n",
        "    <td style=\"border: none; vertical-align: middle; width: 120px;\">\n",
        "      <a href=\"https://hubs.la/Q040tvsK0\">\n",
        "        <img src=\"https://raw.githubusercontent.com/peremartra/Rearchitecting-LLMs/main/Images/cover.png\" width=\"110\" style=\"border-radius: 4px;\">\n",
        "      </a>\n",
        "    </td>\n",
        "    <td style=\"border: none; vertical-align: middle;\">\n",
        "      <h2 style=\"margin: 0;\">The Evolution of LLM Pruning</h2>\n",
        "      <p style=\"margin: 5px 0;\">Whant to know more? check <strong>\"Rearchitecting LLMs\"</strong> (Manning Publications).</p>\n",
        "      <p>\n",
        "        <a href=\"https://hubs.la/Q040tvsK0\"><strong>Check the MEAP</strong></a>\n",
        "      </p>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>\n",
        "______"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEwxZCVsoIau"
      },
      "source": [
        "# Introduction\n",
        "This notebook cotinues the work done at: [6_3_pruning_structured_llama3.2-1b_OK.ipynb](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6-PRUNING/6_3_pruning_structured_llama3.2-1b_OK.ipynb) a width pruning was applied to a Llama3.2 model.\n",
        "\n",
        "In this notebook, we will look at an example of depth pruning, which involves removing entire layers from the model.\n",
        "\n",
        "The first thing to note is that removing entire layers from a transformer model usually has a significant impact on the model's performance. This is a much more drastic architectural change compared to the simple removal of neurons from the MLP layers, as seen in the previous example.\n",
        "\n",
        "For this reason, these models are not designed to be used directly after the pruning process. Instead, they will require a subsequent fine-tuning process to recover their capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQIxAOPZtPBN"
      },
      "source": [
        "#Install libraries & Configure variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5zHApVm41HWq"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q torch\n",
        "!pip install -q datasets\n",
        "!pip install -q sentencepiece  # Required for LLaMA tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GJNgRj4M187E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbIyUlXEtbqs",
        "outputId": "7e0d5a55-6a21-4d15-83f0-60659728a9c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM-QwxyKw-YG"
      },
      "source": [
        "#Download model and explore structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-z_1Zpg2I6u"
      },
      "outputs": [],
      "source": [
        "model_name = 'meta-llama/Llama-3.2-1B'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#tokenizer.pad_token = tokenizer.eos_token  # Set pad token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9UpMD4Hw2MWg"
      },
      "outputs": [],
      "source": [
        "def get_output(prompt, model=model, tokenizer=tokenizer):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "    outputs = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        attention_mask=inputs['attention_mask'],\n",
        "        max_length=50,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        temperature=None,\n",
        "        top_p=None,\n",
        "        do_sample=False,          # Disable sampling\n",
        "        num_beams=5,              # Use beam search\n",
        "        early_stopping=True,      # Stop when end-of-sequence token is generated\n",
        "        no_repeat_ngram_size=2    # Prevent repetition of 2-grams\n",
        "    )\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4muyx_8M5OAu"
      },
      "source": [
        "## studying the model structure\n",
        "As demonstrated in the [previous notebook](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6_2_pruning_structured_llama3.2-1b_KO.ipynb), studying the structure of the model that will undergo pruning is crucial.\n",
        "\n",
        "In this notebook, we’re going to fine-tune the pruning process for the Llama3.2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5Hs4oQ4B7Z0",
        "outputId": "09bc2bfc-d9b4-4a35-aa9d-4ea2112731bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 2048)\n",
            "    (layers): ModuleList(\n",
            "      (0-15): 16 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPMslK3QCAb1"
      },
      "source": [
        "\n",
        "Each layer of the model consists of the attention section and the MLP section. When we remove a layer, the entire layer is eliminated. This is a crucial point because finding a balance in selecting which layers to remove will be challenging.\n",
        "\n",
        "Later, in future notebooks, you will learn about different techniques for selecting layers, based on their activation and how the model responds to a specific dataset.\n",
        "\n",
        "In this notebook, you will explore three different layer selection techniques:\n",
        "\n",
        "- Summing the magnitudes of all the weights in the layers.\n",
        "- Removing the first layers of the model.\n",
        "- Removing the last layers of the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alKH3QH64WFL",
        "outputId": "169f5fe1-3751-4409-ebe4-ebbc8764ac7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text: Paris is the capital of France and one of the most visited cities in the world. It is a city with a rich history and culture, as well as a vibrant and diverse population. Paris is home to many famous landmarks, including the Eiff\n"
          ]
        }
      ],
      "source": [
        "# Test the original model\n",
        "prompt = \"Paris is the capital of\"\n",
        "generated = get_output(prompt)\n",
        "print(f\"Generated text: {generated}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8WR96iwq2XYH"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kph43oObnet7",
        "outputId": "8085e7ed-d1d2-464b-a733-8f9ac145d0d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original model parameters: 1235814400\n"
          ]
        }
      ],
      "source": [
        "original_param_count = count_parameters(model)\n",
        "print(f\"Original model parameters: {original_param_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK9NwmBWnkSP"
      },
      "source": [
        "#Pruning the Model.\n",
        "##Support pruning functions.\n",
        "\n",
        "Here are three differeten methods I used to calculate wich layers to mantain.\n",
        "\n",
        "The function prune_layers calculates the weight magnitude of each layer to remove those that, in principle, should contribute less to the model.\n",
        "\n",
        "The other two functions focus on removing either the initial layers or the final layers.\n",
        "\n",
        "The results obtained with 20% pruning using each method have been quite different:\n",
        "\n",
        "* **prune_layers**: Generated text after pruning: Paris is the capital of & && &ththhth hhh h h shhs h th h f h % h t h\n",
        "\n",
        " h m  h   m\n",
        "  sh  *\n",
        " n\n",
        "* **prune_last_layers**: Paris is the capital of France and arguably one amongstworld renowned cities worldwide. Its uniqueness lies in its uniqueness itselfwhich makes it uniquely unique amongstotherworld renown cities globally.Its uniqueness resides mainly inits unique architecturewhichmakes itunique amongst otherworld\n",
        "* **prune_first_layer**: Paris is the capital of & && &ththhth hhh h h shhs h th h f h % h t h\n",
        " h m  h   m\n",
        "  sh  *\n",
        " n\n",
        "* **Base model**: Paris is the capital of France and one of the most visited cities in the world. It is a city with a rich history and culture, as well as a vibrant and diverse population. Paris is home to many famous landmarks, including the Eiff\n",
        "\n",
        "It is clear that the impact on the model has been quite catastrophic in all cases. The only model that managed to generate text—which, while not entirely coherent, at least resembles text—was the one where the final layers were removed.\n",
        "\n",
        "In a Transformer model like this, layers are organized hierarchically: the initial layers (closer to the input) tend to capture basic language patterns (such as syntactic structure, common word combinations, etc.), while the intermediate and final layers refine these representations, capturing higher-order relationships, global coherence, and subtle semantic nuances.\n",
        "\n",
        "Removing the initial layers directly undermines the foundation upon which more complex representations are built, leading the model to generate meaningless text sequences. Similarly, removing layers based on weight importance metrics (without considering their position or function) can eliminate layers critical for linguistic cohesion or contextual coherence.\n",
        "\n",
        "On the other hand, removing the final layers, while resulting in a loss of some refinement and specialization capabilities, preserves the initial and middle layers that have already learned fundamental language rules and basic word dependencies.\n",
        "\n",
        "However, this is just an empirical and highly simple test. Later, when we evaluate the model's performance using rankings, we will see that it retains a significant portion of its characteristics. Therefore, we are dealing with a model that can deliver very good results after a small fine-tuning process to recover some of the lost capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Seyqaquj7mQA"
      },
      "outputs": [],
      "source": [
        "#DISCARTED: Eliminate layers with smaller absolute values of all parameters.\n",
        "def compute_layer_importance(layer):\n",
        "    \"\"\"\n",
        "    Compute the importance score of a layer by considering the sum of\n",
        "    the absolute values of all its parameters (including attention and MLP).\n",
        "\n",
        "    Args:\n",
        "    - layer: A model layer (e.g., LlamaDecoderLayer).\n",
        "\n",
        "    Returns:\n",
        "    - layer_importance: A scalar importance score for the entire layer.\n",
        "    \"\"\"\n",
        "    # Initialize a tensor for cumulative sum of absolute parameters\n",
        "    device = layer.parameters().__next__().device\n",
        "    layer_importance = torch.tensor(0.0, device=device)\n",
        "\n",
        "    # Accumulate the absolute values of all parameters in the layer\n",
        "    for param in layer.parameters():\n",
        "        layer_importance += torch.sum(torch.abs(param))\n",
        "\n",
        "    return layer_importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "irDWnV5bTwXW"
      },
      "outputs": [],
      "source": [
        "###\n",
        "# DISCARTED: Eliminate layers with smaller absolute values of all parameters.\n",
        "##\n",
        "def prune_layers(model, prune_percent):\n",
        "    \"\"\"\n",
        "    Removes entire layers from the model based on their importance scores.\n",
        "    Now considers all parameters in a layer (attention + MLP) for determining importance.\n",
        "\n",
        "    Args:\n",
        "    - model: The model from which layers will be pruned.\n",
        "    - prune_percent: The percentage of layers to remove.\n",
        "\n",
        "    Returns:\n",
        "    - model: The pruned model with fewer layers.\n",
        "    \"\"\"\n",
        "    # Calculate the importance of each layer and store (index, importance)\n",
        "    layer_importances = []\n",
        "    for idx, layer in enumerate(model.model.layers):\n",
        "        importance = compute_layer_importance(layer)\n",
        "        layer_importances.append((idx, importance))\n",
        "\n",
        "    # Sort layers by importance in ascending order (lowest importance first)\n",
        "    layer_importances.sort(key=lambda x: x[1])\n",
        "\n",
        "    # Compute the number of layers to prune\n",
        "    total_layers = len(layer_importances)\n",
        "    num_layers_to_prune = int(total_layers * prune_percent)\n",
        "\n",
        "    # Get the indices of layers to remove\n",
        "    layers_to_remove = set([x[0] for x in layer_importances[:num_layers_to_prune]])\n",
        "\n",
        "    # Rebuild the model without the removed layers\n",
        "    new_layers = [layer for i, layer in enumerate(model.model.layers) if i not in layers_to_remove]\n",
        "    model.model.layers = nn.ModuleList(new_layers)\n",
        "\n",
        "    return model\n",
        "\n",
        "#Generated text after pruning: Paris is the capital of & && &ththhth hhh h h shhs h th h f h % h t h\n",
        "#\n",
        "# h m  h   m\n",
        "#  sh  *\n",
        "#*\n",
        "# n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NX9Boph94RWA"
      },
      "outputs": [],
      "source": [
        "##############\n",
        "## SELECTED ##\n",
        "##############\n",
        "# ELIMINATE LAST LAYERS OF THE MODEL.\n",
        "def prune_last_layers(model, num_layers_to_remove):\n",
        "    \"\"\"\n",
        "    Removes the last 'num_layers_to_remove' layers from the model.\n",
        "\n",
        "    Args:\n",
        "    - model: The model from which layers will be pruned.\n",
        "    - num_layers_to_remove: Number of layers to remove from the top of the stack.\n",
        "\n",
        "    Returns:\n",
        "    - model: The pruned model with fewer layers.\n",
        "    \"\"\"\n",
        "    total_layers = len(model.model.layers)\n",
        "\n",
        "    # Ensure we are not removing more layers than exist\n",
        "    if num_layers_to_remove >= total_layers:\n",
        "        raise ValueError(\"Number of layers to remove is greater or equal to total layers.\")\n",
        "\n",
        "    # Slice the layers to remove the last ones\n",
        "    new_layers = model.model.layers[:total_layers - num_layers_to_remove]\n",
        "    model.model.layers = nn.ModuleList(new_layers)\n",
        "\n",
        "    # Update the model configuration\n",
        "    model.config.num_hidden_layers = len(model.model.layers)\n",
        "\n",
        "    return model\n",
        "#response:  Paris is the capital of France and arguably one amongstworld renowned cities worldwide. Its uniqueness lies in its uniqueness itselfwhich makes it uniquely unique amongstotherworld renown cities globally.Its uniqueness resides mainly inits unique architecturewhichmakes itunique amongst otherworld"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "r24kGVUCBykp"
      },
      "outputs": [],
      "source": [
        "###\n",
        "# DISCARTED: Eliminate first model layers.\n",
        "##\n",
        "def prune_first_layers(model, num_layers_to_remove):\n",
        "    \"\"\"\n",
        "    Removes the first 'num_layers_to_remove' layers from the model.\n",
        "\n",
        "    Args:\n",
        "    - model: The model from which layers will be pruned.\n",
        "    - num_layers_to_remove: Number of layers to remove from the start.\n",
        "\n",
        "    Returns:\n",
        "    - model: The pruned model with fewer layers.\n",
        "    \"\"\"\n",
        "    # Get the total number of layers in the model.\n",
        "    total_layers = len(model.model.layers)\n",
        "\n",
        "    # Ensure we are not removing more layers than exist\n",
        "    if num_layers_to_remove >= total_layers:\n",
        "        raise ValueError(\"Number of layers to remove is greater or equal to total layers.\")\n",
        "\n",
        "    # Keep all layers after the first 'num_layers_to_remove'\n",
        "    new_layers = model.model.layers[num_layers_to_remove:]\n",
        "    model.model.layers = nn.ModuleList(new_layers)\n",
        "\n",
        "    # Update the model configuration after pruning layers\n",
        "    model.config.num_hidden_layers = len(model.model.layers)\n",
        "\n",
        "    return model\n",
        "\n",
        "#response: Paris is the capital of & && &ththhth hhh h h shhs h th h f h % h t h\n",
        "#\n",
        "# h m  h   m\n",
        "#  sh  *\n",
        "#*\n",
        "# n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT0v_RpeST87"
      },
      "source": [
        "# Prune Loop\n",
        "The update_model function iterates through the blocks within the model's Transformer structure. This structure consists of multiple `LlamaDecoderLayer` blocks, and each of these blocks contains a pair of `LlamaSdpaAttention` and `LlamaMLP` components.\n",
        "```\n",
        "(layers): ModuleList(\n",
        "      (0-15): 16 x LlamaDecoderLayer(\n",
        "        (self_attn): LlamaSdpaAttention(\n",
        "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
        "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
        "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
        "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
        "          (rotary_emb): LlamaRotaryEmbedding()\n",
        "        )\n",
        "        (mlp): LlamaMLP(\n",
        "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
        "          (act_fn): SiLU()\n",
        "        )\n",
        "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "      )\n",
        "  )    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FxJEWg1X3j0m"
      },
      "outputs": [],
      "source": [
        "def update_model(model, prune_percent):\n",
        "    \"\"\"\n",
        "    Modifies the model by removing entire layers from the end of the stack,\n",
        "    instead of basing it on importance scores. This is a heuristic approach\n",
        "    that tries pruning the top layers to see if it yields better results.\n",
        "\n",
        "    Args:\n",
        "    - model: Model to prune.\n",
        "    - prune_percent: Percentage of layers to prune from the top.\n",
        "\n",
        "    Returns:\n",
        "    - model: New pruned model with fewer layers.\n",
        "    \"\"\"\n",
        "    ### uncomment this if you want to use weight layer selection.\n",
        "    #model = prune_layers(model, prune_percent)\n",
        "\n",
        "    total_layers = len(model.model.layers)\n",
        "    num_layers_to_remove = int(total_layers * prune_percent)\n",
        "\n",
        "    model = prune_last_layers(model, num_layers_to_remove)\n",
        "    #model = prune_first_layers(model, num_layers_to_remove)\n",
        "\n",
        "    # Update the model configuration to reflect the new number of layers\n",
        "    model.config.num_hidden_layers = len(model.model.layers)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtHtSbRmS267"
      },
      "source": [
        "## Obtain & test the pruned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NIUnFU5R3n42"
      },
      "outputs": [],
      "source": [
        "prune_percent = 0.2  # Prune 20% of neurons\n",
        "model = update_model(model, prune_percent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdJUkfWI3qMM",
        "outputId": "1cbaca00-e983-42ca-8cca-90eaec0578cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pruned model parameters: 1053349888\n",
            "Reduction in parameters: 182464512\n",
            "Percentage of weight savings: 14.76%\n"
          ]
        }
      ],
      "source": [
        "# Recalculate the number of parameters\n",
        "pruned_param_count = count_parameters(model)\n",
        "reduction_in_params = original_param_count - pruned_param_count\n",
        "percentage_savings = (reduction_in_params / original_param_count) * 100\n",
        "\n",
        "print(f\"Pruned model parameters: {pruned_param_count}\")\n",
        "print(f\"Reduction in parameters: {reduction_in_params}\")\n",
        "print(f\"Percentage of weight savings: {percentage_savings:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvj-iIsO5M6U",
        "outputId": "7bcb9c45-0274-44ea-9662-3413e66c2ee8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text after pruning: Paris is the capital of France and arguably one amongstworld renowned cities worldwide. Its uniqueness lies in its uniqueness itselfwhich makes it uniquely unique amongstotherworld renown cities globally.Its uniqueness resides mainly inits unique architecturewhichmakes itunique amongst otherworld\n"
          ]
        }
      ],
      "source": [
        "# Test the pruned model\n",
        "generated = get_output(prompt, model, tokenizer)\n",
        "print(f\"Generated text after pruning: {generated}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGzXMQrVTULv"
      },
      "source": [
        "The result is realy different from what the original model produced, and is far to be a fairly accurate response, but at least is understable text.\n",
        "\n",
        "In contrast to the model created in notebook: [6_2_pruning_structured_llama3.2-1b_KO.ipynb](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6_2_pruning_structured_llama3.2-1b_KO.ipynb) where the pruned Llama model lost almost all its utility, the model in this notebook retains a good portion of its knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDQrSrf-VCyI"
      },
      "source": [
        "Looking at the model’s new structure, we can see that now the model have only 13 layers instead of the 16 original. So we removed 3 entire layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATAiqZW30NYN",
        "outputId": "94dc2853-d775-4547-e3e7-42b065fe9a14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(128256, 2048)\n",
            "    (layers): ModuleList(\n",
            "      (0-12): 13 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6qEmvooZycx"
      },
      "source": [
        "#Upload the model to HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2Ll_kqe5QzO"
      },
      "outputs": [],
      "source": [
        "new_model_name = 'depth20-llama-3.2-1b'\n",
        "output_dir = './'+new_model_name\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"Pruned model saved to {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LjjsGZV5ZHJ"
      },
      "outputs": [],
      "source": [
        "# Push the model to your Hugging Face repository\n",
        "\n",
        "model.push_to_hub(new_model_name, private=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xNN-aYa5h9B"
      },
      "outputs": [],
      "source": [
        "tokenizer.push_to_hub(new_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdKFR5Ju23kI"
      },
      "source": [
        "#Evaluating models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UM2pkqFAYEe"
      },
      "source": [
        "In this section, we'll take a look at some standard evaluations in the world of Large Language Models using the lm-evaluation library from EleutherAI.\n",
        "\n",
        "Specifically, we'll use LAMBADA, ARC_EASY and BoolQ. Since the pruning performed could be considered structural—that is, it affects the model's overall structure without a specific target—I’ve chosen rather different evaluation tasks.\n",
        "\n",
        "I want to remind you that the goal of this notebook is to demonstrate the pruning process, so I won’t be doing a comprehensive study of how it impacts performance; that will be saved for a future article. Additionally, these models are designed to be fine-tuned before being used.\n",
        "\n",
        "However, I believe that seeing how pruning impacts model performance can help illustrate the pruning process itself.\n",
        "\n",
        "The model selected for the comparision is the one with a 20% pruning using the last layers selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPOg8Hoa22xA"
      },
      "outputs": [],
      "source": [
        "!pip install -q lm-eval\n",
        "from lm_eval import evaluator, tasks, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5wp8lM63IGz"
      },
      "outputs": [],
      "source": [
        "def evaluate_hf_model(model_name, tasks=['arc_easy'], num_fewshot=0):\n",
        "    \"\"\"\n",
        "    It calls the evaluator to evaluate a model available on Hugging Face.\n",
        "\n",
        "    Args:\n",
        "    - model_name: The model name in hugging Face.\n",
        "    - tasks: Tasks to evaluate.\n",
        "    - num_fewshot: Number of examples of few-shot learning\n",
        "\n",
        "    Returns:\n",
        "    - metrics.\n",
        "    \"\"\"\n",
        "    model_args = f\"pretrained={model_name},device=cuda\"\n",
        "    tasks = tasks\n",
        "\n",
        "    results = evaluator.simple_evaluate(\n",
        "      model=\"hf\",\n",
        "      model_args=model_args,\n",
        "      tasks=tasks,\n",
        "      num_fewshot=0,  # Number of few-shot smaples.\n",
        "      limit=None,  # Use all the samples in the Evaluate Dataset.\n",
        "      bootstrap_iters=10\n",
        "    )\n",
        "\n",
        "    metrics = results.get('results', {})\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZm8VvA33Nh6"
      },
      "outputs": [],
      "source": [
        "# Select tasks to evaluate.\n",
        "tasks = ['lambada', 'boolq', 'arc_easy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTW3E3mp145e"
      },
      "outputs": [],
      "source": [
        "metrics_pruned = evaluate_hf_model(\"oopere/depth20-llama-3.2-1b\", tasks=tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-g3vyPN3VZp"
      },
      "outputs": [],
      "source": [
        "metrics_pruned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhAmm3oH4DPg"
      },
      "outputs": [],
      "source": [
        "metrics_base= evaluate_hf_model(\"meta-llama/Llama-3.2-1B\", tasks=tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bboB7uU39l_Z"
      },
      "outputs": [],
      "source": [
        "metrics_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWNOHoVdpgcP"
      },
      "source": [
        "![My Image](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/img/depth_rpunedvsbase.png?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c1FodzbrwMv"
      },
      "source": [
        "**BoolQ:** The performance drop is minimal (from 0.64 to 0.63). BoolQ is a dataset of boolean (true/false) questions based on reading comprehension. The fact that the model nearly maintains its performance suggests that the information required to answer these questions is less sensitive to layer removal or that this type of task primarily benefits from more basic language information, which remains intact after pruning.\n",
        "\n",
        "**Lambada (OpenAI and Standard):** Here, the performance drop is much more pronounced. For Lambada OpenAI, the score decreases from 0.62 to 0.46, and for Lambada Standard, it drops from 0.53 to 0.11—a drastic reduction. Lambada focuses on predicting the last word in a text, requiring deep contextual understanding and long-term coherence. This result aligns with the earlier test in the notebook, where the model was asked to complete a prompt, and the generated language was not entirely accurate. This difficulty in producing coherent text within a given context negatively impacted its performance in the evaluated task.\n",
        "\n",
        "**ARC Easy:** This benchmark also shows a significant decline (from 0.65 to 0.46). ARC Easy is a reasoning benchmark focused on general knowledge and common sense. The removal of layers likely impacted the model's ability to relate information and maintain reasoning chains, resulting in a reduced capacity to select the correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MwoMVHLZ7ad"
      },
      "source": [
        "#Conclusion.\n",
        "\n",
        "In this notebook, we explored how depth pruning works on Llama-3.2 models. Unlike width pruning, there was no need to account for the functioning of the structure.\n",
        "\n",
        "After pruning, the model experiences significant degradation in tasks that require greater contextual and semantic reasoning (Lambada, ARC Easy), while its performance on BoolQ, a comparatively simpler task, remains almost unchanged. This suggests that pruning disproportionately affects the parts of the model that facilitate complex understanding and long-term coherence. BoolQ, being simpler or less dependent on these traits, remains relatively stable, whereas tasks that evaluate context and global coherence are severely impacted.\n",
        "\n",
        "As indicated in the paper: What Matters in Transformers? What Matters in Transformers? Not All Attention is Needed. https://arxiv.org/abs/2406.15786 the best result is achieved by removing the deepest layers of the model.\n",
        "\n",
        "\n",
        "## Future Work.\n",
        "\n",
        "So far, we have explored two forms of structured pruning:\n",
        "\n",
        "- Width pruning: In this approach, neurons from the MLP layers of two model families, DistilGPT and Llama3, were removed. The process of removing neurons from models with GLU architecture works across all families with this structure, such as QWEN, Gemma, or Microsoft Phi.\n",
        "- Depth pruning: Entire blocks were removed from a Llama3 model. This technique can be adapted to all model families.\n",
        "\n",
        "The common point is that we have used very similar methods to decide which elements to remove from the models, based on the absolute weight of the parameters. The next step will involve making these decisions based on metrics generated while the model is running. This will allow us to create models tailored to specific datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW-biFT6U0zQ"
      },
      "source": [
        "##Authors Note.\n",
        "In addition to creating content like this notebook and offering it under the MIT license, I have also contributed to repositories such as those of Hugging Face and Google Gemini.\n",
        "\n",
        "I am especially proud of my book: <a href=\"https://amzn.to/4eanT1g\"><b>Large Language Models:</b> Apply and Implement Strategies for Large Language Models</a> (Apress).\n",
        "\n",
        "You can find it on both <a href=\"https://amzn.to/4eanT1g\">Amazon</a> and <a href=\"https://link.springer.com/book/10.1007/979-8-8688-0515-8\">Springer</a>, where they often have good deals on the purchase price.\n",
        "\n",
        "If you take a look and end up purchasing it, keep in mind that you can reach out with any questions via the Discussions section of this same repository or on any of my social media channels. I’ll do my best to respond as quickly as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcE2aGubuYDC"
      },
      "source": [
        "##Authors Note.\n",
        "In addition to creating content like this notebook and offering it under the MIT license, I have also contributed to repositories such as those of Hugging Face and Google Gemini.\n",
        "\n",
        "I am especially proud of my book: <a href=\"https://hubs.la/Q040tvsK0\"><b>Rearchitecting LLMs - Structural techniques for efficient models</b></a> (Manning Publications).\n",
        "\n",
        "You can find it at Manning.\n",
        "\n",
        "If you take a look and end up purchasing it, keep in mind that you can reach out with any questions via the Discussions section of this same repository or on any of my social media channels. I’ll do my best to respond as quickly as possible."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
