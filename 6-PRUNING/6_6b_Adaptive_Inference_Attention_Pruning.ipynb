{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Large-Language-Model-Notebooks-Course/blob/inference-adaptative-attention-pruning/6-PRUNING/6_6b_Adaptive_Inference_Attention_Pruning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV5KPbfseL8S"
      },
      "source": [
        "<div>\n",
        "    <h1>Large Language Models Projects</a></h1>\n",
        "    <h3>Apply and Implement Strategies for Large Language Models</h3>\n",
        "    <h2>Pruning Attention Layers</h2>\n",
        "    <h3>Not All Attention is needed</h3>\n",
        "</div>\n",
        "\n",
        "by [Pere Martra](https://www.linkedin.com/in/pere-martra/)\n",
        "\n",
        "_______\n",
        "Models: meta-llama/Llama-3.2\n",
        "\n",
        "Colab Environment: GPU L4 for 3B Models\n",
        "\n",
        "T4 for 1B Model.\n",
        "\n",
        "Keys:\n",
        "* Pruning\n",
        "* Attention\n",
        "\n",
        "References:\n",
        "* [What Matters in Transformers? Not All Attention is Needed](https://arxiv.org/abs/2406.15786)\n",
        "* [Resource-Efficient Transformer Pruning for Finetuning of Large Models](https://openaccess.thecvf.com/content/CVPR2024/html/Ilhan_Resource-Efficient_Transformer_Pruning_for_Finetuning_of_Large_Models_CVPR_2024_paper.html)\n",
        "\n",
        "_______\n",
        "**disclaimer: The pruning / knowledge distillation section has been created after the first edition of the book was published. They are not included in the bookâ€™s original content but are intended to supplement and expand on the topics covered.**\n",
        "\n",
        "This is the unofficial repository for the book:\n",
        "        <a href=\"https://amzn.to/4eanT1g\"> <b>Large Language Models:</b> Apply and Implement Strategies for Large Language Models</a> (Apress).\n",
        "        The book is based on the content of this repository, but the notebooks are being updated, and I am incorporating new examples and chapters.\n",
        "        If you are looking for the official repository for the book, with the original notebooks, you should visit the\n",
        "        <a href=\"https://github.com/Apress/Large-Language-Models-Projects\">Apress repository</a>, where you can find all the notebooks in their original format as they appear in the book.\n",
        "\n",
        "______\n",
        "# Introduction\n",
        "This notebook implements the paper: [What Matters in Transformers? Not all Attention is Needed](https://arxiv.org/abs/2406.15786).\n",
        "Although I followed the paper's guidelines, I made some adjustments to make the code clearer and easier to understand.\n",
        "\n",
        "The original paper demonstrates that larger models tend to have excessive redundancy in their attention layers. They achieved a 48.4% increase in inference performance for a Llama-2-70B model with only a minor 2.4% drop in response quality, **just bypassing the 50% of the Attention layers!**\n",
        "\n",
        "In this notebook, tests have been conducted using Llama-3.2-1B and 3B models. With these models, I found that removing 50% of the attention layers significantly impacted the model's functionality. However, the 3B model handled the removal of these layers much better. This suggests that redundancy may become more pronounced as model size increases.\n",
        "\n",
        "# Methodology.\n",
        "To identify which layers contribute the least, the cosine distance between the layer's input and output is measured. In the paper, this distance is calculated using a test dataset, while in the notebook, I used a simple prompt to activate the layers.  \n",
        "\n",
        "This method of measuring the importance of attention layers and their contribution to the model allows pruning to be tailored to a specific dataset. This approach can lead to the creation of more efficient models for specialized sectors such as healthcare or finance.\n",
        "\n",
        "\n",
        "Once the layer contributing the least to the final output is identified (the one with the smallest difference between input and output), it is added to a list included in the configuration file.\n",
        "\n",
        "This list is then referenced during inference by a new forward function that replaces the original one for attention layers. When this new function detects that a layer is in the list, it skips its execution and simply returns the input without modifications.\n",
        "\n",
        "The process of identifying layers to deactivate and marking them as non-executable is one-shot. In other words, it does  determine all the layers to skip in one go, as recommended in the paper.\n",
        "\n",
        "The iterative implementation has a significant drawback: the test dataset must be processed for each layer to be deactivated. The paper's authors note that while the iterative method may bring slight improvements, the added computational cost is not justified. However, since this is an example notebook, and there is no test datasetâ€”just a small promptâ€”and the layer selection process takes only seconds, I chose the iterative approach.\n",
        "\n",
        "This pruning method does not produce a smaller model, as the layers are not physically removed. They remain in the model but are not executed, resulting in improved inference response times.\n",
        "______"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwYeKwswnkTG"
      },
      "source": [
        "# Install libraries & Configure variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PblPrYCiYTl8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b5f26431-2ce1-412c-a3c2-13949042f099"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hf_xet\n",
            "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
            "Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf_xet\n",
            "Successfully installed hf_xet-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torch==2.6.0\n",
        "!pip install -q torchvision==0.21.0\n",
        "!pip install -q transformers==4.51.3\n",
        "!pip install -q datasets==3.6.0\n",
        "!pip install -q lm-eval==0.4.8\n",
        "\n",
        "!pip install hf_xet #To speed up downloads from HF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6qN0mu6IHqpy"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "from copy import deepcopy\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptative Configuration."
      ],
      "metadata": {
        "id": "hYdRIDoV8R3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# ADAPTIVE ATTENTION BYPASS (AAB) CONFIGURATION - CORRECTED SCALING TO 100%\n",
        "# =============================================================================\n",
        "\n",
        "GLOBAL_COMPLEXITIES = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "ADAPTIVE_CONFIG = {\n",
        "    # Model size-based ratios with proportional scaling to 100%\n",
        "    \"model_size_ratios\": {\n",
        "        \"70B+\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.15, \"scaling_factor\": 0.85},\n",
        "            \"simple\": {\"min_ratio\": 0.35, \"scaling_factor\": 0.65},\n",
        "            \"medium\": {\"min_ratio\": 0.55, \"scaling_factor\": 0.45},\n",
        "            \"complex\": {\"min_ratio\": 0.75, \"scaling_factor\": 0.25},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        },\n",
        "        \"30B-70B\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.25, \"scaling_factor\": 0.75},\n",
        "            \"simple\": {\"min_ratio\": 0.40, \"scaling_factor\": 0.60},\n",
        "            \"medium\": {\"min_ratio\": 0.60, \"scaling_factor\": 0.40},\n",
        "            \"complex\": {\"min_ratio\": 0.80, \"scaling_factor\": 0.20},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        },\n",
        "        \"10B-30B\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.30, \"scaling_factor\": 0.75},\n",
        "            \"simple\": {\"min_ratio\": 0.45, \"scaling_factor\": 0.55},\n",
        "            \"medium\": {\"min_ratio\": 0.65, \"scaling_factor\": 0.35},\n",
        "            \"complex\": {\"min_ratio\": 0.82, \"scaling_factor\": 0.18},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        },\n",
        "        \"5B-10B\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.35, \"scaling_factor\": 0.65},\n",
        "            \"simple\": {\"min_ratio\": 0.55, \"scaling_factor\": 0.45},\n",
        "            \"medium\": {\"min_ratio\": 0.75, \"scaling_factor\": 0.25},\n",
        "            \"complex\": {\"min_ratio\": 0.87, \"scaling_factor\": 0.13},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        },\n",
        "        \"2B-5B\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.60, \"scaling_factor\": 0.40},\n",
        "            \"simple\": {\"min_ratio\": 0.75, \"scaling_factor\": 0.20},\n",
        "            \"medium\": {\"min_ratio\": 0.90, \"scaling_factor\": 0.20},\n",
        "            \"complex\": {\"min_ratio\": 0.95, \"scaling_factor\": 0.10},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        },\n",
        "        \"<2B\": {\n",
        "            \"trivial\": {\"min_ratio\": 0.80, \"scaling_factor\": 0.20},\n",
        "            \"simple\": {\"min_ratio\": 0.85, \"scaling_factor\": 0.15},\n",
        "            \"medium\": {\"min_ratio\": 0.90, \"scaling_factor\": 0.10},\n",
        "            \"complex\": {\"min_ratio\": 0.95, \"scaling_factor\": 0.05},\n",
        "            \"very_complex\": {\"min_ratio\": 1.0, \"scaling_factor\": 0.0}\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # 5-level complexity thresholds and descriptions\n",
        "    \"complexity_levels\": {\n",
        "        \"trivial\": {\n",
        "            \"range\": [0.0, 0.2],\n",
        "            \"description\": \"Single word answers, basic arithmetic\",\n",
        "            \"layer_groups\": [\"imprescindibles\", \"critical\"]\n",
        "        },\n",
        "        \"simple\": {\n",
        "            \"range\": [0.2, 0.4],\n",
        "            \"description\": \"Simple factual questions\",\n",
        "            \"layer_groups\": [\"imprescindibles\", \"critical\", \"important\"]\n",
        "        },\n",
        "        \"medium\": {\n",
        "            \"range\": [0.4, 0.6],\n",
        "            \"description\": \"Knowledge retrieval, completion tasks\",\n",
        "            \"layer_groups\": [\"imprescindibles\", \"critical\", \"important\", \"standard\"]\n",
        "        },\n",
        "        \"complex\": {\n",
        "            \"range\": [0.6, 0.8],\n",
        "            \"description\": \"Reasoning, explanations\",\n",
        "            \"layer_groups\": [\"imprescindibles\", \"critical\", \"important\", \"standard\", \"optional\"]\n",
        "        },\n",
        "        \"very_complex\": {\n",
        "            \"range\": [0.8, 1.0],\n",
        "            \"description\": \"Deep reasoning, analysis, creativity\",\n",
        "            \"layer_groups\": [\"imprescindibles\", \"critical\", \"important\", \"standard\", \"optional\", \"dispensable\"]\n",
        "        }\n",
        "    },\n",
        "\n",
        "    # Layer activation thresholds (1:1 correspondence)\n",
        "    \"activation_thresholds\": {\n",
        "        \"critical\": 0.0,      # Always active above trivial level\n",
        "        \"important\": 0.2,     # Active from simple level\n",
        "        \"standard\": 0.4,      # Active from medium level\n",
        "        \"optional\": 0.6,      # Active from complex level\n",
        "        \"dispensable\": 0.8    # Active from very_complex level\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"âœ… CORRECTED 5-level adaptive configuration loaded!\")\n",
        "print(f\"ğŸ“Š All models can now reach 100% layers for very_complex prompts\")\n",
        "print(f\"ğŸ¯ 5 complexity levels: {list(ADAPTIVE_CONFIG['complexity_levels'].keys())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "feRLhLsw8Voz",
        "outputId": "ce70f2a7-6c98-4fa3-f7ac-f125bd7a0f54"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… CORRECTED 5-level adaptive configuration loaded!\n",
            "ğŸ“Š All models can now reach 100% layers for very_complex prompts\n",
            "ğŸ¯ 5 complexity levels: ['trivial', 'simple', 'medium', 'complex', 'very_complex']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support & calculate functions"
      ],
      "metadata": {
        "id": "tlPEOEaj8b7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_model_size_category(model):\n",
        "    \"\"\"\n",
        "    Automatically detect model size category from model parameters\n",
        "    \"\"\"\n",
        "    try:\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        size_billion = total_params / 1e9\n",
        "\n",
        "        print(f\"ğŸ” Detected model size: {size_billion:.2f}B parameters\")\n",
        "\n",
        "        if size_billion >= 70:\n",
        "            return \"70B+\"\n",
        "        elif size_billion >= 30:\n",
        "            return \"30B-70B\"\n",
        "        elif size_billion >= 10:\n",
        "            return \"10B-30B\"\n",
        "        elif size_billion >= 5:\n",
        "            return \"5B-10B\"\n",
        "        elif size_billion >= 2:\n",
        "            return \"2B-5B\"\n",
        "        else:\n",
        "            return \"<2B\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Error detecting model size: {e}\")\n",
        "        return \"1B-3B\"\n",
        "\n",
        "\n",
        "def get_context_window_size(model, tokenizer=None):\n",
        "    \"\"\"\n",
        "    Tries to detect the model's maximum context window size (in tokens).\n",
        "    \"\"\"\n",
        "    # Try model config first (most reliable)\n",
        "    if hasattr(model, 'config'):\n",
        "        config = model.config\n",
        "\n",
        "        # Common attribute names for context window\n",
        "        context_attrs = ['max_position_embeddings', 'n_positions', 'max_seq_len',\n",
        "                        'seq_length', 'max_sequence_length', 'context_length']\n",
        "\n",
        "        for attr in context_attrs:\n",
        "            if hasattr(config, attr):\n",
        "                value = getattr(config, attr)\n",
        "                if isinstance(value, int) and value > 0:\n",
        "                    return value\n",
        "\n",
        "    # Try tokenizer as fallback\n",
        "    if tokenizer:\n",
        "        if hasattr(tokenizer, 'model_max_length') and tokenizer.model_max_length != int(1e30):\n",
        "            return tokenizer.model_max_length\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def count_attention_layers_correctly(model):\n",
        "    \"\"\"\n",
        "    Correctly count attention layers by finding main decoder/transformer layers\n",
        "    \"\"\"\n",
        "    # Method 1: Count main decoder layers directly (most reliable)\n",
        "    decoder_layer_count = 0\n",
        "    for name, module in model.named_modules():\n",
        "        module_type = type(module).__name__\n",
        "        # Look for main transformer/decoder layers\n",
        "        if any(layer_type in module_type for layer_type in\n",
        "               ['DecoderLayer', 'TransformerBlock', 'Block', 'Layer']) and \\\n",
        "           any(exclude not in module_type for exclude in\n",
        "               ['Embedding', 'Norm', 'Linear', 'MLP', 'Attention']):\n",
        "            # Make sure it's a numbered layer (e.g., layers.0, layers.1, etc.)\n",
        "            if '.layers.' in name and name.count('.') == 2:  # e.g., \"model.layers.0\"\n",
        "                decoder_layer_count += 1\n",
        "\n",
        "    if decoder_layer_count > 0:\n",
        "        return decoder_layer_count\n",
        "\n",
        "    # Method 2: Use model config as fallback\n",
        "    try:\n",
        "        if hasattr(model, 'config'):\n",
        "            config_attrs = ['num_hidden_layers', 'n_layer', 'num_layers', 'n_layers']\n",
        "            for attr in config_attrs:\n",
        "                if hasattr(model.config, attr):\n",
        "                    return getattr(model.config, attr)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Method 3: Direct access to layers ModuleList\n",
        "    try:\n",
        "        if hasattr(model, 'model') and hasattr(model.model, 'layers'):\n",
        "            return len(model.model.layers)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return 16  # Conservative fallback\n",
        "\n",
        "\n",
        "def classify_complexity_level(complexity_score):\n",
        "    \"\"\"\n",
        "    Classify complexity score into one of 5 levels\n",
        "\n",
        "    Args:\n",
        "        complexity_score (float): Complexity score (0.0-1.0)\n",
        "\n",
        "    Returns:\n",
        "        str: Complexity level (\"trivial\", \"simple\", \"medium\", \"complex\", \"very_complex\")\n",
        "    \"\"\"\n",
        "    levels = ADAPTIVE_CONFIG[\"complexity_levels\"]\n",
        "\n",
        "    for level_name, level_config in levels.items():\n",
        "        min_val, max_val = level_config[\"range\"]\n",
        "        if min_val <= complexity_score < max_val:\n",
        "            return level_name\n",
        "\n",
        "    # Handle edge case for exactly 1.0\n",
        "    if complexity_score >= 0.8:\n",
        "        return \"very_complex\"\n",
        "\n",
        "    return \"trivial\"  # Fallback\n",
        "\n",
        "\n",
        "def calculate_active_layers(total_layers, model_size_category, complexity_score):\n",
        "    \"\"\"\n",
        "    Calculate number of active layers based on complexity and model size\n",
        "\n",
        "    Args:\n",
        "        total_layers (int): Total number of attention layers\n",
        "        model_size_category (str): Model size category\n",
        "        complexity_score (float): Complexity score (0.0-1.0)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (active_layers_count, complexity_level, layer_groups_used, min_guaranteed, max_possible)\n",
        "    \"\"\"\n",
        "    # Classify complexity level\n",
        "    complexity_level = classify_complexity_level(complexity_score)\n",
        "\n",
        "    # Get configuration for this model size and complexity\n",
        "    config = ADAPTIVE_CONFIG[\"model_size_ratios\"][model_size_category][complexity_level]\n",
        "    min_ratio = config[\"min_ratio\"]\n",
        "    scaling_factor = config[\"scaling_factor\"]\n",
        "\n",
        "    # Calculate layer counts\n",
        "    min_guaranteed = int(total_layers * min_ratio)\n",
        "    remaining_layers = total_layers - min_guaranteed\n",
        "    additional_layers = int(complexity_score * scaling_factor * remaining_layers)\n",
        "    active_layers = min_guaranteed + additional_layers\n",
        "\n",
        "    # Ensure we don't exceed total layers\n",
        "    active_layers = min(active_layers, total_layers)\n",
        "    max_possible = total_layers  # Always can reach 100%\n",
        "\n",
        "    # Get which layer groups should be used\n",
        "    layer_groups_used = ADAPTIVE_CONFIG[\"complexity_levels\"][complexity_level][\"layer_groups\"]\n",
        "\n",
        "    return active_layers, complexity_level, layer_groups_used, min_guaranteed, max_possible\n",
        "\n",
        "\n",
        "def get_complexity_info(complexity_level):\n",
        "    \"\"\"\n",
        "    Get detailed information about a complexity level\n",
        "    \"\"\"\n",
        "    return ADAPTIVE_CONFIG[\"complexity_levels\"][complexity_level]\n",
        "\n",
        "\n",
        "print(\"âœ… Enhanced 5-level calculation functions loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7n-xWD2o8hmA",
        "outputId": "21f8652f-b049-48c3-fc27-6333bdbdd643"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Enhanced 5-level calculation functions loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8iRr5iapy5q"
      },
      "source": [
        "# Download the Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "u9IFTYa9P6Zy",
        "outputId": "9e9049f1-af19-485f-83f3-73d62ca72443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbnDW_tZRTGp"
      },
      "outputs": [],
      "source": [
        "#model_name = 'meta-llama/Llama-3.2-1B'\n",
        "model_name = 'meta-llama/Llama-3.2-3B'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#tokenizer.pad_token = tokenizer.eos_token  # Set pad token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "TkQnkc3U8l-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the configuration with clean, simplified output\n",
        "if 'model' in locals():\n",
        "    # Get model information with improved detection\n",
        "    total_attention_layers = count_attention_layers_correctly(model)\n",
        "    model_category = detect_model_size_category(model)\n",
        "    context_window = get_context_window_size(model, tokenizer if 'tokenizer' in locals() else None)\n",
        "\n",
        "    print(f\"\\nğŸ—ï¸ Model Analysis:\")\n",
        "    print(f\"   Attention layers: {total_attention_layers}\")\n",
        "    print(f\"   Size category: {model_category}\")\n",
        "    print(f\"   Context window: {context_window if context_window else 'Unknown'} tokens\")\n",
        "    print(f\"   Architecture: {type(model).__name__}\")\n",
        "\n",
        "    # Show layer detection verification\n",
        "    print(f\"\\nğŸ” Layer Detection Verification:\")\n",
        "    decoder_layers = [name for name, module in model.named_modules()\n",
        "                     if 'DecoderLayer' in type(module).__name__ and '.layers.' in name]\n",
        "    print(f\"   Found DecoderLayers: {len(decoder_layers)}\")\n",
        "\n",
        "    # Test all 5 complexity levels with simplified table\n",
        "    test_complexities = GLOBAL_COMPLEXITIES\n",
        "\n",
        "    print(\"\\nğŸ§ª Layer Activation by Complexity Level:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"{'Level':<12} {'Active Layers':<15} {'Usage Ratio':<12}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for complexity in test_complexities:\n",
        "        active, level, groups, min_guaranteed, max_possible = calculate_active_layers(\n",
        "            total_attention_layers, model_category, complexity\n",
        "        )\n",
        "        ratio = active / total_attention_layers\n",
        "\n",
        "        print(f\"{level.capitalize():<12} {active:<15} {ratio:<12.1%}\")\n",
        "\n",
        "    print(f\"\\nğŸ“Š Summary for {model_category} model:\")\n",
        "    trivial_config = ADAPTIVE_CONFIG['model_size_ratios'][model_category]['trivial']\n",
        "    trivial_min = int(total_attention_layers * trivial_config['min_ratio'])\n",
        "    print(f\"   â€¢ Range: {trivial_min}-{total_attention_layers} layers ({trivial_min/total_attention_layers:.1%}-100%)\")\n",
        "    print(f\"   â€¢ Context window: {context_window if context_window else 'Unknown'} tokens\")\n",
        "    print(f\"   â€¢ All complexity levels can reach 100% layer usage\")\n",
        "\n",
        "else:\n",
        "    print(\"â³ Load your model first to test the configuration\")\n",
        "    print(\"\\nTo test, make sure you have:\")\n",
        "    print(\"1. model = ... (your loaded model)\")\n",
        "    print(\"2. tokenizer = ... (optional, your tokenizer)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5rUlhopw8s63",
        "outputId": "799461e8-fb3a-4a17-e27e-dc319e3e51fc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Detected model size: 3.21B parameters\n",
            "\n",
            "ğŸ—ï¸ Model Analysis:\n",
            "   Attention layers: 28\n",
            "   Size category: 2B-5B\n",
            "   Context window: 131072 tokens\n",
            "   Architecture: LlamaForCausalLM\n",
            "\n",
            "ğŸ” Layer Detection Verification:\n",
            "   Found DecoderLayers: 28\n",
            "\n",
            "ğŸ§ª Layer Activation by Complexity Level:\n",
            "==================================================\n",
            "Level        Active Layers   Usage Ratio \n",
            "--------------------------------------------------\n",
            "Trivial      16              57.1%       \n",
            "Simple       21              75.0%       \n",
            "Medium       25              89.3%       \n",
            "Complex      26              92.9%       \n",
            "Very_complex 28              100.0%      \n",
            "\n",
            "ğŸ“Š Summary for 2B-5B model:\n",
            "   â€¢ Range: 16-28 layers (57.1%-100%)\n",
            "   â€¢ Context window: 131072 tokens\n",
            "   â€¢ All complexity levels can reach 100% layer usage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0ifeungrbHw"
      },
      "source": [
        "## Study the structure.\n",
        "* Llama-3.2-1B\n",
        "```\n",
        "LlamaForCausalLM(\n",
        "  (model): LlamaModel(\n",
        "    (embed_tokens): Embedding(128256, 2048)\n",
        "    (layers): ModuleList(\n",
        "      (0-15): 16 x LlamaDecoderLayer(\n",
        "        (self_attn): LlamaSdpaAttention(\n",
        "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
        "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
        "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
        "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
        "          (rotary_emb): LlamaRotaryEmbedding()\n",
        "        )\n",
        "        (mlp): LlamaMLP(\n",
        "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
        "          (act_fn): SiLU()\n",
        "        )\n",
        "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "      )\n",
        "    )\n",
        "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "    (rotary_emb): LlamaRotaryEmbedding()\n",
        "  )\n",
        "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "The model follows the typical structure of modern Llama models, consisting of blocks made up of an Attention layer and an MLP layer with a GLU structure.\n",
        "\n",
        "> If you want to see an example of how to perform pruning on the MLP layers of the model, you can check out the notebook:[Pruning Llama 3.2.](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6-PRUNING/6_3_pruning_structured_llama3.2-1b_OK.ipynb) y leer el paper [Exploring GLU expansion ratios: Structured pruning in Llama-3.2 models](https://osf.io/preprints/osf/qgxea)\n",
        "\n",
        "\n",
        "\n",
        "Since the layers form a block, the attention layer cannot be removed without also removing the accompanying MLP layer. For this reason, the decision was made to deactivate their execution during inference.\n",
        "\n",
        "The 1B model has 16 layers, as shown in the structure above, while the 3B model has 28 layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference function & Test Base Model\n",
        "\n",
        "The `get_output` function is designed to generate text  and measure the time taken for different stages of the generation process.\n",
        "\n",
        "It provides insights into the performance of the model and can be used to evaluate the efficiency of text generation."
      ],
      "metadata": {
        "id": "vF4cHUb_rICs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2igvy4z6rGgy"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def get_output(prompt, model=model, tokenizer=tokenizer, num_runs=1, max_length=50):\n",
        "    total_time = 0\n",
        "    generated_outputs = []\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        # Start timing\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Tokenization time\n",
        "        token_start = time.time()\n",
        "        inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "        token_time = time.time() - token_start\n",
        "\n",
        "        # Generation time\n",
        "        gen_start = time.time()\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            temperature=None,\n",
        "            top_p=None,\n",
        "            do_sample=False,  # Disable sampling\n",
        "            num_beams=5,      # Use beam search\n",
        "            early_stopping=True,  # Stop when end-of-sequence token is generated\n",
        "            no_repeat_ngram_size=2  # Prevent repetition of 2-grams\n",
        "        )\n",
        "        gen_time = time.time() - gen_start\n",
        "\n",
        "        # Decoding time\n",
        "        decode_start = time.time()\n",
        "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        decode_time = time.time() - decode_start\n",
        "\n",
        "        # Total time for this run\n",
        "        total_time += time.time() - start_time\n",
        "        generated_outputs.append(generated)\n",
        "\n",
        "        if num_runs > 1:\n",
        "            print(f\"\\nRun {run + 1}:\")\n",
        "        print(f\"Tokenization time: {token_time*1000:.2f} ms\")\n",
        "        print(f\"Generation time: {gen_time*1000:.2f} ms\")\n",
        "        print(f\"Decoding time: {decode_time*1000:.2f} ms\")\n",
        "        print(f\"Total time: {(time.time() - start_time)*1000:.2f} ms\")\n",
        "\n",
        "    if num_runs > 1:\n",
        "        avg_time = total_time / num_runs\n",
        "        print(f\"\\nAverage time over {num_runs} runs: {avg_time*1000:.2f} ms\")\n",
        "\n",
        "    return generated_outputs[0] if num_runs == 1 else generated_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1354c66e-aef6-405e-fe21-3bec5afa2773",
        "id": "lH7cotAxrhO3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run 1:\n",
            "Tokenization time: 3.10 ms\n",
            "Generation time: 4195.37 ms\n",
            "Decoding time: 0.32 ms\n",
            "Total time: 4198.89 ms\n",
            "\n",
            "Run 2:\n",
            "Tokenization time: 0.65 ms\n",
            "Generation time: 3044.44 ms\n",
            "Decoding time: 0.23 ms\n",
            "Total time: 3045.43 ms\n",
            "\n",
            "Average time over 2 runs: 3622.05 ms\n",
            "Generated text: ['Paris is the capital of France. It is located in the north-central part of the country, on the river Seine. The city has a population of over 2 million people, making it the largest city in France and the second-largest city', 'Paris is the capital of France. It is located in the north-central part of the country, on the river Seine. The city has a population of over 2 million people, making it the largest city in France and the second-largest city']\n"
          ]
        }
      ],
      "source": [
        "# Test the original model\n",
        "prompt = \"Paris is the capital of\"\n",
        "generated = get_output(prompt, num_runs=2)\n",
        "print(f\"Generated text: {generated}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text generation of the original model, as expected, works perfectly and returns a correct and meaningful sentence."
      ],
      "metadata": {
        "id": "mo4IjOYGry0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cpu\")               # actual data moves â†™\n",
        "torch.cuda.empty_cache()      # allocator drops cached blocks"
      ],
      "metadata": {
        "id": "bLN1_gLdt7Rx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjgf4WA_vF3B"
      },
      "source": [
        "# Pruning the Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TQDCkoL6RW3C"
      },
      "outputs": [],
      "source": [
        "#import torch\n",
        "#import torch.nn as nn\n",
        "#from torch.nn import functional as F\n",
        "#from copy import deepcopy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOuKyH0MjyJM"
      },
      "source": [
        "## Execute Pruning."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Disclaimer**\n",
        "\n",
        "I'm using a single illustrative prompt so that the code path is easy to follow. In any research or production setting you must feed hundreds or thousands of diverse prompts before deciding which layers to deactivate"
      ],
      "metadata": {
        "id": "wyI5XiqYyBnc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HvhSYyE2Rk1H"
      },
      "outputs": [],
      "source": [
        "# Using multiple prompts for calibration\n",
        "calibration_prompts = [\n",
        "    \"Hi I'm a sample text, used to calculate the cosine difference between input and output.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Machine learning models can be optimized through various techniques, explain the principals.\",\n",
        "    \"2+2=\",\n",
        "    \"What is the meaning of life, the universe, and everything?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SIMPLE AAB CALIBRATION - MINIMAL CODE\n",
        "# =============================================================================\n",
        "def measure_layer_importance_simple(model, tokenizer, prompts):\n",
        "    \"\"\"Simple layer importance measurement - FIXED using original notebook pattern\"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    total_layers = len(model.model.layers)\n",
        "\n",
        "    # Accumulate importance scores across all prompts\n",
        "    importance_acc = {idx: 0.0 for idx in range(total_layers)}\n",
        "\n",
        "    print(f\"ğŸ“Š Processing {len(prompts)} prompts across {total_layers} layers...\")\n",
        "\n",
        "    for prompt_idx, prompt in enumerate(prompts):\n",
        "        print(f\"   Processing prompt {prompt_idx + 1}/{len(prompts)}\")\n",
        "\n",
        "        # Tokenize input (following original notebook pattern)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Storage for this prompt's layer inputs/outputs\n",
        "        layer_inputs = {}\n",
        "        layer_outputs = {}\n",
        "\n",
        "        # Create hooks (EXACTLY like the original function)\n",
        "        def q_proj_input_hook(layer_idx):\n",
        "            def _hook(module, module_input):\n",
        "                # Handle tuple input (following original pattern)\n",
        "                inp = module_input[0] if isinstance(module_input, tuple) else module_input\n",
        "                layer_inputs[layer_idx] = inp.detach().clone()\n",
        "            return _hook\n",
        "\n",
        "        def o_proj_output_hook(layer_idx):\n",
        "            def _hook(module, module_input, module_output):\n",
        "                # Handle tuple output (following original pattern)\n",
        "                out = module_output[0] if isinstance(module_output, tuple) else module_output\n",
        "                layer_outputs[layer_idx] = out.detach().clone()\n",
        "            return _hook\n",
        "\n",
        "        # Register hooks for ALL layers (not just unpruned ones)\n",
        "        handles = []\n",
        "        for idx in range(total_layers):\n",
        "            layer = model.model.layers[idx]\n",
        "            handles.append(layer.self_attn.q_proj.register_forward_pre_hook(q_proj_input_hook(idx)))\n",
        "            handles.append(layer.self_attn.o_proj.register_forward_hook(o_proj_output_hook(idx)))\n",
        "\n",
        "        # Forward pass (following original pattern)\n",
        "        with torch.no_grad():\n",
        "            _ = model(**inputs)\n",
        "\n",
        "        # Remove hooks (following original pattern)\n",
        "        for h in handles:\n",
        "            h.remove()\n",
        "\n",
        "        # Calculate importance for each layer (EXACTLY like original)\n",
        "        for idx in range(total_layers):\n",
        "            if idx in layer_inputs and idx in layer_outputs:\n",
        "                inp = layer_inputs[idx]\n",
        "                out = layer_outputs[idx]\n",
        "\n",
        "                # Flatten tensors (following original pattern)\n",
        "                inp_flat = inp.view(inp.size(0), -1)\n",
        "                out_flat = out.view(out.size(0), -1)\n",
        "\n",
        "                # Calculate similarity and importance (following original pattern)\n",
        "                similarity = F.cosine_similarity(inp_flat, out_flat, dim=1).mean().item()\n",
        "                importance_score = 1 - similarity\n",
        "                importance_acc[idx] += importance_score\n",
        "\n",
        "    # Average across all prompts\n",
        "    avg_importance = {idx: importance_acc[idx] / len(prompts) for idx in range(total_layers)}\n",
        "\n",
        "    print(\"âœ… Layer importance measurement complete!\")\n",
        "    return avg_importance\n",
        "\n",
        "\n",
        "def create_adaptive_config_simple(model, tokenizer, prompts):\n",
        "    \"\"\"Create OPTIMIZED adaptive config - ultra-simple format for efficient inference\"\"\"\n",
        "    print(\"ğŸš€ Creating optimized adaptive config...\")\n",
        "\n",
        "    # Step 1: Analyze model\n",
        "    model_size_category = detect_model_size_category(model)\n",
        "    total_layers = count_attention_layers_correctly(model)\n",
        "    context_window = get_context_window_size(model, tokenizer)\n",
        "\n",
        "    # Step 2: Measure importance\n",
        "    print(\"ğŸ“Š Measuring layer importance...\")\n",
        "    importance_scores = measure_layer_importance_simple(model, tokenizer, prompts)\n",
        "\n",
        "    # Step 3: Create layers_by_importance (sorted list)\n",
        "    print(\"ğŸ† Creating layers_by_importance list...\")\n",
        "    sorted_layers = sorted(importance_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    layers_by_importance = [layer_idx for layer_idx, _ in sorted_layers]\n",
        "\n",
        "    # Step 4: Calculate complexity thresholds using existing notebook functions\n",
        "    print(\"ğŸ¯ Calculating complexity thresholds...\")\n",
        "    complexity_scores = GLOBAL_COMPLEXITIES\n",
        "    complexity_thresholds = {}\n",
        "\n",
        "    print(\"ğŸ“Š Using notebook functions to get exact layer counts:\")\n",
        "    for score in complexity_scores:\n",
        "        active_layers_count, _, _, _, _ = calculate_active_layers(\n",
        "            total_layers, model_size_category, score\n",
        "        )\n",
        "        complexity_thresholds[score] = active_layers_count\n",
        "        level_name = classify_complexity_level(score)\n",
        "        print(f\"   Score {score:3.1f} ({level_name:12}) â†’ {active_layers_count:2d}/{total_layers} layers\")\n",
        "\n",
        "    # Step 5: Build OPTIMIZED config\n",
        "    print(\"âš™ï¸ Building optimized configuration...\")\n",
        "    config = {\n",
        "        \"model_info\": {\n",
        "            \"name\": getattr(model.config, '_name_or_path', 'unknown'),\n",
        "            \"total_parameters\": f\"{sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\",\n",
        "            \"size_category\": model_size_category,\n",
        "            \"total_layers\": total_layers,\n",
        "            \"context_window\": context_window,\n",
        "            \"architecture\": type(model).__name__\n",
        "        },\n",
        "        \"layers_by_importance\": layers_by_importance,\n",
        "        \"complexity_thresholds\": complexity_thresholds\n",
        "    }\n",
        "\n",
        "    # Step 6: Save optimized config\n",
        "    with open(\"adaptive_config.json\", \"w\") as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    print(\"âœ… OPTIMIZED adaptive_config.json created!\")\n",
        "\n",
        "    # Show optimized results\n",
        "    print(f\"ğŸ“Š Model: {total_layers} layers, {model_size_category}\")\n",
        "    print(f\"ğŸ† Layers by importance: {layers_by_importance[:5]}... (showing first 5)\")\n",
        "    print(\"ğŸ¯ Complexity thresholds:\")\n",
        "    for threshold, count in complexity_thresholds.items():\n",
        "        percentage = (count / total_layers) * 100\n",
        "        level = classify_complexity_level(threshold)\n",
        "        print(f\"   {threshold:3.1f} ({level:12}): {count:2d} layers ({percentage:4.1f}%)\")\n",
        "\n",
        "    print(\"\\nğŸš€ ULTRA-EFFICIENT RUNTIME FORMAT:\")\n",
        "    print(\"   â€¢ No redundant data\")\n",
        "    print(\"   â€¢ Direct score â†’ layer count lookup\")\n",
        "    print(\"   â€¢ Minimal JSON size\")\n",
        "    print(\"   â€¢ Fastest possible inference decisions\")\n",
        "\n",
        "    return config\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6yrcseaPSBpb",
        "outputId": "db6dfed2-2d27-4763-d7b2-881e7697a372"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ CREATING ULTRA-EFFICIENT ADAPTIVE CONFIG\n",
            "==================================================\n",
            "ğŸš€ Creating optimized adaptive config...\n",
            "ğŸ” Detected model size: 3.21B parameters\n",
            "ğŸ“Š Measuring layer importance...\n",
            "ğŸ“Š Processing 5 prompts across 28 layers...\n",
            "   Processing prompt 1/5\n",
            "   Processing prompt 2/5\n",
            "   Processing prompt 3/5\n",
            "   Processing prompt 4/5\n",
            "   Processing prompt 5/5\n",
            "âœ… Layer importance measurement complete!\n",
            "ğŸ† Creating layers_by_importance list...\n",
            "ğŸ¯ Calculating complexity thresholds...\n",
            "ğŸ“Š Using notebook functions to get exact layer counts:\n",
            "   Score 0.1 (trivial     ) â†’ 16/28 layers\n",
            "   Score 0.3 (simple      ) â†’ 21/28 layers\n",
            "   Score 0.5 (medium      ) â†’ 25/28 layers\n",
            "   Score 0.7 (complex     ) â†’ 26/28 layers\n",
            "   Score 0.9 (very_complex) â†’ 28/28 layers\n",
            "âš™ï¸ Building optimized configuration...\n",
            "âœ… OPTIMIZED adaptive_config.json created!\n",
            "ğŸ“Š Model: 28 layers, 2B-5B\n",
            "ğŸ† Layers by importance: [9, 8, 12, 10, 7]... (showing first 5)\n",
            "ğŸ¯ Complexity thresholds:\n",
            "   0.1 (trivial     ): 16 layers (57.1%)\n",
            "   0.3 (simple      ): 21 layers (75.0%)\n",
            "   0.5 (medium      ): 25 layers (89.3%)\n",
            "   0.7 (complex     ): 26 layers (92.9%)\n",
            "   0.9 (very_complex): 28 layers (100.0%)\n",
            "\n",
            "ğŸš€ ULTRA-EFFICIENT RUNTIME FORMAT:\n",
            "   â€¢ No redundant data\n",
            "   â€¢ Direct score â†’ layer count lookup\n",
            "   â€¢ Minimal JSON size\n",
            "   â€¢ Fastest possible inference decisions\n",
            "\n",
            "ğŸ‰ DONE! Optimized adaptive_config.json ready for AAB!\n",
            "==================================================\n",
            "ğŸ“‹ What's in your optimized config:\n",
            "   âœ… model_info - Essential model metadata\n",
            "   âœ… layers_by_importance - Ordered list [most â†’ least important]\n",
            "   âœ… complexity_thresholds - Direct score â†’ layer count mapping\n",
            "   âŒ NO redundant data (groups, rankings, ratios)\n",
            "   âŒ NO unnecessary metadata\n",
            "\n",
            "ğŸ’¡ Runtime efficiency:\n",
            "   â€¢ JSON size: ~80% smaller\n",
            "   â€¢ Lookup time: O(1) direct access\n",
            "   â€¢ Memory usage: Minimal\n",
            "   â€¢ Perfect for production inference!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# SIMPLE EXECUTION - OPTIMIZED VERSION\n",
        "# =============================================================================\n",
        "\n",
        "print(\"ğŸš€ CREATING ULTRA-EFFICIENT ADAPTIVE CONFIG\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create the OPTIMIZED adaptive config using existing calibration_prompts\n",
        "adaptive_config = create_adaptive_config_simple(model, tokenizer, calibration_prompts)\n",
        "\n",
        "print(f\"\\nğŸ‰ DONE! Optimized adaptive_config.json ready for AAB!\")\n",
        "print(\"=\" * 50)\n",
        "print(\"ğŸ“‹ What's in your optimized config:\")\n",
        "print(\"   âœ… model_info - Essential model metadata\")\n",
        "print(\"   âœ… layers_by_importance - Ordered list [most â†’ least important]\")\n",
        "print(\"   âœ… complexity_thresholds - Direct score â†’ layer count mapping\")\n",
        "print(\"   âŒ NO redundant data (groups, rankings, ratios)\")\n",
        "print(\"   âŒ NO unnecessary metadata\")\n",
        "print(f\"\\nğŸ’¡ Runtime efficiency:\")\n",
        "print(\"   â€¢ JSON size: ~80% smaller\")\n",
        "print(\"   â€¢ Lookup time: O(1) direct access\")\n",
        "print(\"   â€¢ Memory usage: Minimal\")\n",
        "print(\"   â€¢ Perfect for production inference!\")"
      ],
      "metadata": {
        "id": "XRuB-fZ6VK5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adaptive_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ExBmu0i5AMvx",
        "outputId": "cf2bd944-77d0-4538-ea7e-4c513b63f2f4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_info': {'name': 'meta-llama/Llama-3.2-3B',\n",
              "  'total_parameters': '3.21B',\n",
              "  'size_category': '2B-5B',\n",
              "  'total_layers': 28,\n",
              "  'context_window': 131072,\n",
              "  'architecture': 'LlamaForCausalLM'},\n",
              " 'layers_by_importance': [9,\n",
              "  8,\n",
              "  12,\n",
              "  10,\n",
              "  7,\n",
              "  0,\n",
              "  6,\n",
              "  27,\n",
              "  13,\n",
              "  18,\n",
              "  11,\n",
              "  5,\n",
              "  14,\n",
              "  3,\n",
              "  4,\n",
              "  2,\n",
              "  15,\n",
              "  1,\n",
              "  21,\n",
              "  17,\n",
              "  25,\n",
              "  24,\n",
              "  16,\n",
              "  22,\n",
              "  20,\n",
              "  26,\n",
              "  23,\n",
              "  19],\n",
              " 'complexity_thresholds': {0.1: 16, 0.3: 21, 0.5: 25, 0.7: 26, 0.9: 28}}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46_tIN1brKZW"
      },
      "source": [
        "# Test Pruned Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFdCesVcnnrM"
      },
      "source": [
        "Now, let's test the pruned model, which is a Llama-3.2-3B model where I have marked 4 Attention layers to be bypassed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oCJ8EYLhJg-",
        "outputId": "8ecaa3b1-5d72-44ba-b033-d82d70eb097f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run 1:\n",
            "Tokenization time: 0.46 ms\n",
            "Generation time: 1251.89 ms\n",
            "Decoding time: 0.23 ms\n",
            "Total time: 1252.68 ms\n",
            "\n",
            "Run 2:\n",
            "Tokenization time: 0.59 ms\n",
            "Generation time: 1245.58 ms\n",
            "Decoding time: 0.21 ms\n",
            "Total time: 1246.48 ms\n",
            "\n",
            "Average time over 2 runs: 1249.48 ms\n",
            "Generated text: ['Paris is the capital of France and/or world-wide fame for its beautiful skyline skyline-top-top-bottom-bottomside-side-side sidesidednessnessNESSNESSnessinessinessnessesivenessivenessfulnessnessfulnessfulnessinessesenessesinesssinessESness', 'Paris is the capital of France and/or world-wide fame for its beautiful skyline skyline-top-top-bottom-bottomside-side-side sidesidednessnessNESSNESSnessinessinessnessesivenessivenessfulnessnessfulnessfulnessinessesenessesinesssinessESness']\n"
          ]
        }
      ],
      "source": [
        "# Test the pruned model\n",
        "pruned_model = pruned_model.to(device) #Move the model to GPU again.\n",
        "generated = get_output(prompt, pruned_model, num_runs=2)\n",
        "print(f\"Generated text: {generated}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ic4ux7goGu_"
      },
      "source": [
        "\n",
        "The execution of this second model is slightly faster than that of the base model, and the generated text is fairly accurate, although some repetition can be noticed towards the end of the sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Store the Model.\n"
      ],
      "metadata": {
        "id": "H7R_VaIPTQhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_model_name = 'attnprun-llama-3.2-3B'\n",
        "output_dir = './'+new_model_name\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "pruned_model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "#new_config.save_pretrained(output_dir)\n",
        "print(f\"Pruned model saved to {output_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERXYQsy5TUtn",
        "outputId": "50e669d9-04ed-40bd-835e-463ed60a3d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned model saved to ./attnprun-llama-3.2-3B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Check that config contains layers to skip\n",
        "from transformers import AutoConfig\n",
        "config = AutoConfig.from_pretrained(output_dir)\n",
        "\n",
        "if hasattr(config, \"drop_attn_list\"):\n",
        "    print(f\"drop_attn_list stored: {config.drop_attn_list}\")\n",
        "else:\n",
        "    print(\"drop_attn_list isn't present.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdE7o0PETggr",
        "outputId": "66718550-f72e-47d2-b2d3-a96d1cb04c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drop_attn_list stored: [14, 13, 12, 11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload to Hugging Face."
      ],
      "metadata": {
        "id": "hG63t8jqVdOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El proceso de subida de este modelo a Hugging es ligeramente mÃ¡s complejo por que se debe almacenar no tan solo el modelo en si, sino tambien el cÃ³digo de la funciÃ³n _bypass_single_layer. Que como recordarÃ¡s es la funciÃ³n que se encarga de decidir cuando ejecutar o simplemente bypasear una capa de atenciÃ³n.  "
      ],
      "metadata": {
        "id": "YRDoj5bYKWVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, upload_folder, whoami"
      ],
      "metadata": {
        "id": "0bi3zX7FVjwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Get your HF username from the current token\n",
        "username = whoami()[\"name\"]  # Returns a dict like {'name': 'your_username', 'email': ...}\n",
        "username"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_AnSaJPuWffg",
        "outputId": "01f84a9a-6d3c-469f-9cf2-67225de8a998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'oopere'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define repo name\n",
        "repo_id = f\"{username}/{new_model_name}\""
      ],
      "metadata": {
        "id": "KDuY1p8zWjax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define path to your model\n",
        "output_dir = \"./\"+new_model_name\n"
      ],
      "metadata": {
        "id": "-MNgDvImWmFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function must be saved in a .py file, but since this notebook runs on Colab, Iâ€™ve decided the best approach is to create a cell that generates the file to be uploaded.\n",
        "\n",
        "The file contains the custom class PrunedLlamaForCausalLM, which extends Hugging Faceâ€™s LlamaForCausalLM.\n",
        "\n",
        "This custom class calls the base constructor, ensuring that the model's configuration file includes the drop_attn_list, which specifies the layers that should be skipped.\n",
        "\n",
        "The forward function is modified only for the layers that need to be skipped; the rest continue executing their standard forward function.\n"
      ],
      "metadata": {
        "id": "dJSb5z2nLRS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_model_code = '''\n",
        "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
        "\n",
        "class PrunedLlamaForCausalLM(LlamaForCausalLM):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        if not hasattr(config, \"drop_attn_list\"):\n",
        "            config.drop_attn_list = []\n",
        "\n",
        "        for idx in config.drop_attn_list:\n",
        "            self._bypass_single_layer(idx)\n",
        "\n",
        "    def _bypass_single_layer(self, layer_idx):\n",
        "        \"\"\"\n",
        "        Modifies the specified layer's forward method so that attention is bypassed.\n",
        "        \"\"\"\n",
        "        layer = self.model.layers[layer_idx]\n",
        "        if not hasattr(layer.self_attn, \"_original_forward\"):\n",
        "            layer.self_attn._original_forward = layer.self_attn.forward\n",
        "\n",
        "        def new_attention_forward(self, hidden_states, attention_mask=None, position_ids=None,\n",
        "                                  past_key_value=None, output_attentions=False, use_cache=False,\n",
        "                                  **kwargs):\n",
        "            if getattr(self, \"layer_idx\", -1) in self.config.drop_attn_list:\n",
        "                if use_cache:\n",
        "                    return hidden_states, None\n",
        "                else:\n",
        "                    return hidden_states, None\n",
        "            return self._original_forward(hidden_states, attention_mask, position_ids,\n",
        "                                          past_key_value, output_attentions, use_cache, **kwargs)\n",
        "\n",
        "        layer.self_attn.layer_idx = layer_idx\n",
        "        layer.self_attn.forward = new_attention_forward.__get__(layer.self_attn, type(layer.self_attn))\n",
        "\n",
        "'''\n",
        "\n",
        "# Define path and write the file\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "with open(os.path.join(output_dir, \"modeling_attnprun_llama.py\"), \"w\") as f:\n",
        "    f.write(custom_model_code.strip())\n",
        "\n",
        "print(\"Custom model script modeling_attnprun_llama.py created successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_xaG0FDXojc",
        "outputId": "f79b2cf2-59fb-48c6-fece-b2410f91b7cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom model script modeling_attnprun_llama.py created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the model's configuration file is updated by adding the `auto_map` field, which tells the Transformers library which class to use to construct the model: `modeling_attnprun_llama.PrunedLlamaForCausalLM.`\n"
      ],
      "metadata": {
        "id": "r59Yae3jVMpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Path to the config file\n",
        "config_path = os.path.join(output_dir, \"config.json\")\n",
        "\n",
        "# Load the existing config\n",
        "with open(config_path, \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# Add or update the auto_map section\n",
        "config[\"auto_map\"] = {\n",
        "    \"AutoModelForCausalLM\": \"modeling_attnprun_llama.PrunedLlamaForCausalLM\"\n",
        "}\n",
        "\n",
        "# Optional: ensure the architecture field is aligned\n",
        "config[\"architectures\"] = [\"PrunedLlamaForCausalLM\"]\n",
        "\n",
        "# Save the updated config\n",
        "with open(config_path, \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"config.json updated with auto_map and architecture.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBbxbBbmbeRl",
        "outputId": "1105e204-17e1-4cd5-b93e-733cad7b00ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json updated with auto_map and architecture.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time to upload the folder containing the weights, the config file and the new function to HF."
      ],
      "metadata": {
        "id": "0ciXxElsWsQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Upload the folder to the Hub\n",
        "upload_folder(\n",
        "    folder_path=output_dir,\n",
        "    path_in_repo=\"\",  # Upload everything to root\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "print(f\"Model uploaded successfully to https://huggingface.co/{repo_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCUToiTmWp9M",
        "outputId": "e0c99d01-b250-471e-91ef-39959ab662a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model uploaded successfully to https://huggingface.co/oopere/attnprun-llama-3.2-3B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download model from Hugging Face."
      ],
      "metadata": {
        "id": "8Lp3bWMOehLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "del pruned_model\n",
        "del tokenizer\n",
        "del model\n",
        "\n",
        "# 2. Libera la cachÃ© de la GPU\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()  # Opcional, ayuda en Colab\n",
        "\n",
        "# 3. Forza recolecciÃ³n de basura en Python\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2eDLdnEmapS",
        "outputId": "66087f86-ec21-4fa5-a240-a4d78ff549e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "186"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is downloaded normally from Hugging Face, but you must remember to set `trust_remote_code=True` since the model includes the custom code you previously created and uploaded.\n"
      ],
      "metadata": {
        "id": "UgHQ5-2MW6q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_model_name=\"oopere/attnprun-llama-3.2-3B\"\n",
        "\n",
        "model_hf = AutoModelForCausalLM.from_pretrained(\n",
        "    pruned_model_name,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pruned_model_name)"
      ],
      "metadata": {
        "id": "d4AB09pNemii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_hf = model_hf.to(device) #Move the model to GPU again.\n",
        "generated = get_output(prompt, model_hf, num_runs=2)\n",
        "print(f\"Generated text: {generated}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2feKDCrPll0E",
        "outputId": "79f8010b-6fed-4460-c2ea-5442036f7a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run 1:\n",
            "Tokenization time: 0.77 ms\n",
            "Generation time: 1245.88 ms\n",
            "Decoding time: 0.23 ms\n",
            "Total time: 1246.97 ms\n",
            "\n",
            "Run 2:\n",
            "Tokenization time: 0.49 ms\n",
            "Generation time: 1239.63 ms\n",
            "Decoding time: 0.19 ms\n",
            "Total time: 1240.41 ms\n",
            "\n",
            "Average time over 2 runs: 1243.60 ms\n",
            "Generated text: ['Paris is the capital of France and/or world-wide fame for its beautiful skyline skyline-top-top-bottom-bottomside-side-side sidesidednessnessNESSNESSnessinessinessnessesivenessivenessfulnessnessfulnessfulnessinessesenessesinesssinessESness', 'Paris is the capital of France and/or world-wide fame for its beautiful skyline skyline-top-top-bottom-bottomside-side-side sidesidednessnessNESSNESSnessinessinessnessesivenessivenessfulnessnessfulnessfulnessinessesenessesinesssinessESness']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CsaXViPtmft"
      },
      "source": [
        "# Conclusion.\n",
        "Based on the findings in the paper and the results obtained, I believe this type of pruning may work better with larger models where attention layers tend to have redundancy.\n",
        "\n",
        "Since this type of pruning does not alter the model's structure, it does not result in a reduction in its size or the memory required to load it. The main advantage of using this pruning approach is the reduction of computational load during inference, leading to a more efficient model with faster responses and lower resource consumption.\n",
        "\n",
        "Unlike the original paper, which describes \"removing\" selected attention layers but provides limited implementation details, this implementation takes a transparent functional approach by explicitly overriding the `forward` method only in the specified layers. As a result, the model retains its full architecture and parameter set, but selectively skips computations at runtime. This makes the method reversible, modular, and fully compatible with the Hugging Face ecosystem using `trust_remote_code=True`. While both approaches achieve similar computational savings, this one emphasizes clarity, portability, and practical integration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XDtwh-dwMHh"
      },
      "source": [
        "# Authors Note.\n",
        "\n",
        "In addition to creating content like this notebook and offering it under the MIT license, I have also contributed to repositories such as those of Hugging Face and Google Gemini.\n",
        "\n",
        "I am especially proud of my book: [Large Language Models: Apply and Implement Strategies for Large Language Models (Apress)(https://amzn.to/3DSepLb).\n",
        "\n",
        "You can find it on both [Amazon](https://amzn.to/3DSepLb) and [Springer](https://link.springer.com/book/10.1007/979-8-8688-0515-8), where they often have good deals on the purchase price.\n",
        "\n",
        "If you take a look and end up purchasing it, keep in mind that you can reach out with any questions via the Discussions section of this same repository or on any of my social media channels. Iâ€™ll do my best to respond as quickly as possible."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "authorship_tag": "ABX9TyOBNbNdcUA+o6iHPSekyLVm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}